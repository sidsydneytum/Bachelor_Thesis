{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d350e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe4c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -------------------- GPU configuration -------------------- #\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_GPU = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27bafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Config ------------------- #\n",
    "SEED = 0\n",
    "NPS = [50, 100]          # Number of features to select\n",
    "MWS = [10, 20]           # Enhancement windows\n",
    "UPDATERS = [\"direct\", \"greville\", \"updated_greville\"]  # Updater methods to compare\n",
    "MP = 100                 # Enhancement nodes per window\n",
    "NW = 1                   # Feature windows (as in Kellinger MNIST)\n",
    "SPIKE_STEPS = 30         # Number of spike steps\n",
    "RIDGE = 0.0              # Ridge regularization\n",
    "MNIST_PATH = \"./data/mnist\"\n",
    "SAVE_DIR = \"figures/confusionmatrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796b6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Utlities-------------------#\n",
    "def ensure_dirs():\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_onehot(y:np.ndarray, num_classes:int) -> np.ndarray:\n",
    "    y = y.astype(int)\n",
    "    oh = np.zeros((y.shape[0], num_classes), dtype=np.float32)\n",
    "    oh[np.arange(y.shape[0]), y] = 1.0\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea23306",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------Time & Memory-------------##\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.t0 = time.perf_counter()\n",
    "        return self\n",
    "    def __exit__(self, *exc):\n",
    "        self.elapsed = time.perf_counter() - self.t0\n",
    "\n",
    "def peak_mem_mb() -> float:\n",
    "    proc = psutil.Process(os.getpid())\n",
    "    return proc.memory_info().rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec01f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "##----------------sbls helpers-------------##\n",
    "\n",
    "def aggregate_spikes(spk):\n",
    "    \"\"\"Sum over time then per-sample min-max normalize to [0,1]. spk: (N,D,steps) -> (N,D)\"\"\"\n",
    "    s = spk.sum(axis=2)\n",
    "    mn = s.min(axis=1, keepdims=True)\n",
    "    mx = s.max(axis=1, keepdims=True)\n",
    "    return ((s - mn) / (mx - mn + 1e-8)).astype(np.float32)\n",
    "\n",
    "def lif_like(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Lightweight nonlinearity mapped to [0,1].\"\"\"\n",
    "    z = np.tanh(x).astype(np.float32)\n",
    "    mn = z.min(axis=1, keepdims=True)\n",
    "    mx = z.max(axis=1, keepdims=True)\n",
    "    return ((z - mn) / (mx - mn + 1e-8)).astype(np.float32)\n",
    "\n",
    "def rate_code(arr, steps):\n",
    "    # arr in [0,1], out (N,D,steps) with Bernoulli(arr)\n",
    "    N, D = arr.shape\n",
    "    probs = np.repeat(arr[:, :, None], steps, axis=2)\n",
    "    return (np.random.rand(N, D, steps) < probs).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d6873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##----------------pinv_ridge-------------##\n",
    "\n",
    "def pinv_ridge_np(A: np.ndarray, lam: float = 1e-3) -> np.ndarray:\n",
    "    AtA = A.T @ A\n",
    "    G = AtA + lam * np.eye(AtA.shape[0], dtype=A.dtype)\n",
    "    try:\n",
    "        X = np.linalg.solve(G, A.T)\n",
    "    except np.linalg.LinAlgError:\n",
    "        X = np.linalg.pinv(G) @ A.T\n",
    "    return X  # (d x N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600ff77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##----------------plot confusion matrix-----------##\n",
    "\n",
    "def plot_confusionmatrix(cm: np.ndarray, title: str, path: str):\n",
    "    cm = cm.astype(np.float32)\n",
    "    rs = cm.sum(axis=1, keepdims=True)\n",
    "    cm = np.divide(cm, rs, out=np.zeros_like(cm), where=rs!=0)\n",
    "\n",
    "    plt.figure(figsize=(5,4), dpi=140)\n",
    "    plt.imshow(cm, interpolation=\"nearest\")  # no colorbar\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted class\")\n",
    "    plt.ylabel(\"True class\")\n",
    "\n",
    "    ticks = range(cm.shape[0])\n",
    "    plt.xticks(ticks, fontsize=6)\n",
    "    plt.yticks(ticks, fontsize=6)\n",
    "\n",
    "    # annotate cells\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            val = cm[i, j]\n",
    "            plt.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
    "                     color=(\"black\" if val > 0.5 else \"white\"), fontsize=7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a046a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Data Loader (MNIST)-------------------#\n",
    "def load_mnist(flatten: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    tf = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train = datasets.MNIST(MNIST_PATH, train=True, download=True, transform=tf)\n",
    "    mnist_test  = datasets.MNIST(MNIST_PATH, train=False, download=True, transform=tf)\n",
    "\n",
    "    X_train = mnist_train.data.numpy().astype(np.float32) / 255.0  # (60000, 28, 28)\n",
    "    y_train = mnist_train.targets.numpy().astype(np.int64)\n",
    "    X_test  = mnist_test.data.numpy().astype(np.float32) / 255.0   # (10000, 28, 28)\n",
    "    y_test  = mnist_test.targets.numpy().astype(np.int64)\n",
    "\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)  # (N, 784)\n",
    "        X_test  = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719aed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Random Layers-------------------#\n",
    "def init_feature_layer(in_dim, np_per_win, nw, rng):\n",
    "    Ws, bs = [], []\n",
    "    for _ in range(nw):\n",
    "        Ws.append(rng.randn(in_dim, np_per_win).astype(np.float32) * 0.2)\n",
    "        bs.append((rng.rand(np_per_win).astype(np.float32) - 0.5) * 0.1)\n",
    "    return Ws, bs\n",
    "\n",
    "def forward_feature_layer(X, Ws, bs, steps, rng):\n",
    "    outs = []\n",
    "    for W, b in zip(Ws, bs):\n",
    "        lin = X @ W + b[None, :]\n",
    "        # rescale to [0,1], spike, aggregate\n",
    "        mn = lin.min(axis=1, keepdims=True); mx = lin.max(axis=1, keepdims=True)\n",
    "        norm = (lin - mn) / (mx - mn + 1e-8)\n",
    "        spk = rate_code(norm, steps)\n",
    "        outs.append(aggregate_spikes(spk))\n",
    "    return np.concatenate(outs, axis=1).astype(np.float32)\n",
    "\n",
    "def init_enh_layer(feat_dim, mp_per_win, mw, rng):\n",
    "    Ws, bs = [], []\n",
    "    for _ in range(mw):\n",
    "        Ws.append(rng.randn(feat_dim, mp_per_win).astype(np.float32) * 0.2)\n",
    "        bs.append((rng.rand(mp_per_win).astype(np.float32) - 0.5) * 0.1)\n",
    "    return Ws, bs\n",
    "\n",
    "def forward_enh_layer(Z, Ws, bs, steps, rng):\n",
    "    outs = []\n",
    "    for W, b in zip(Ws, bs):\n",
    "        lin = Z @ W + b[None, :]\n",
    "        z = lif_like(lin)\n",
    "        spk = rate_code(z, steps)\n",
    "        outs.append(aggregate_spikes(spk))\n",
    "    return np.concatenate(outs, axis=1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9bdfc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== GPU helpers (Torch) =================== #\n",
    "def rate_code_torch(arr: torch.Tensor, steps: int) -> torch.Tensor:\n",
    "    N, D = arr.shape\n",
    "    probs = arr.unsqueeze(-1).expand(N, D, steps)\n",
    "    return (torch.rand((N, D, steps), device=arr.device, dtype=arr.dtype) < probs).to(arr.dtype)\n",
    "\n",
    "def aggregate_spikes_torch(spk: torch.Tensor) -> torch.Tensor:\n",
    "    s = spk.sum(dim=2)\n",
    "    mn = s.min(dim=1, keepdim=True).values\n",
    "    mx = s.max(dim=1, keepdim=True).values\n",
    "    return (s - mn) / (mx - mn + 1e-8)\n",
    "\n",
    "def lif_like_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    z = torch.tanh(x)\n",
    "    mn = z.min(dim=1, keepdim=True).values\n",
    "    mx = z.max(dim=1, keepdim=True).values\n",
    "    return (z - mn) / (mx - mn + 1e-8)\n",
    "\n",
    "def init_feature_layer_torch(in_dim, np_per_win, nw, rng_np, device):\n",
    "    Ws, bs = [], []\n",
    "    for _ in range(nw):\n",
    "        W = torch.tensor(rng_np.randn(in_dim, np_per_win).astype(np.float32) * 0.2, device=device)\n",
    "        b = torch.tensor((rng_np.rand(np_per_win).astype(np.float32) - 0.5) * 0.1, device=device)\n",
    "        Ws.append(W); bs.append(b)\n",
    "    return Ws, bs\n",
    "\n",
    "def init_enh_layer_torch(feat_dim, mp_per_win, mw, rng_np, device):\n",
    "    Ws, bs = [], []\n",
    "    for _ in range(mw):\n",
    "        W = torch.tensor(rng_np.randn(feat_dim, mp_per_win).astype(np.float32) * 0.2, device=device)\n",
    "        b = torch.tensor((rng_np.rand(mp_per_win).astype(np.float32) - 0.5) * 0.1, device=device)\n",
    "        Ws.append(W); bs.append(b)\n",
    "    return Ws, bs\n",
    "\n",
    "def forward_feature_layer_torch(X: torch.Tensor, Ws: list, bs: list, steps: int) -> torch.Tensor:\n",
    "    outs = []\n",
    "    for W, b in zip(Ws, bs):\n",
    "        lin = X @ W + b.unsqueeze(0)\n",
    "        mn = lin.min(dim=1, keepdim=True).values\n",
    "        mx = lin.max(dim=1, keepdim=True).values\n",
    "        norm = (lin - mn) / (mx - mn + 1e-8)\n",
    "        spk = rate_code_torch(norm, steps)\n",
    "        outs.append(aggregate_spikes_torch(spk))\n",
    "    return torch.cat(outs, dim=1).to(torch.float32)\n",
    "\n",
    "def forward_enh_layer_torch(Z: torch.Tensor, Ws: list, bs: list, steps: int) -> torch.Tensor:\n",
    "    outs = []\n",
    "    for W, b in zip(Ws, bs):\n",
    "        lin = Z @ W + b.unsqueeze(0)\n",
    "        z = lif_like_torch(lin)\n",
    "        spk = rate_code_torch(z, steps)\n",
    "        outs.append(aggregate_spikes_torch(spk))\n",
    "    return torch.cat(outs, dim=1).to(torch.float32)\n",
    "\n",
    "def pinv_ridge_torch(A: torch.Tensor, lam: float = 1e-3) -> torch.Tensor:\n",
    "    AtA = A.T @ A\n",
    "    n = AtA.shape[0]\n",
    "    I = torch.eye(n, dtype=A.dtype, device=A.device)\n",
    "    return torch.linalg.solve(AtA + lam * I, A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19737480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Updaters wrappers-------------------#\n",
    "def extend_columns_with_backend(updater_name: str, Z: np.ndarray, Z_pinv: np.ndarray, H: np.ndarray, lam: float):\n",
    "    \"\"\"\n",
    "    - 'greville' or 'updated_greville': try your updates modules and common function names.\n",
    "    - 'direct': call your updates/direct.py DirectUpdater.add_columns on torch tensors.\n",
    "    - fallback: compute pinv on full A=[Z|H] in numpy.\n",
    "    \"\"\"\n",
    "    if updater_name == \"direct\":\n",
    "        try:\n",
    "            from updates.direct import DirectUpdater\n",
    "            A_old = torch.from_numpy(Z)\n",
    "            A_plus_old = torch.from_numpy(Z_pinv)  # not used by DirectUpdater\n",
    "            H_new = torch.from_numpy(H)\n",
    "            upd = DirectUpdater(lam=lam)\n",
    "            A_aug, A_plus_new = upd.add_columns(A_old, A_plus_old, H_new)\n",
    "            return A_aug.numpy(), A_plus_new.numpy()\n",
    "        except Exception as e:\n",
    "            # fallback to numpy\n",
    "            A = np.concatenate([Z, H], axis=1)\n",
    "            return A, pinv_ridge_np(A, lam)\n",
    "\n",
    "    # greville / updated_greville path\n",
    "    mod = None\n",
    "    try:\n",
    "        if updater_name == \"greville\":\n",
    "            import updates.greville as mod\n",
    "        elif updater_name == \"updated_greville\":\n",
    "            import updates.updated_greville as mod\n",
    "    except Exception:\n",
    "        mod = None\n",
    "\n",
    "    if mod is not None:\n",
    "        for fname in [\"extend_columns\", \"column_partition_pinv\", \"extend\"]:\n",
    "            fn = getattr(mod, fname, None)\n",
    "            if callable(fn):\n",
    "                try:\n",
    "                    A_pinv = fn(Z_pinv, Z, H, ridge=lam)\n",
    "                except TypeError:\n",
    "                    A_pinv = fn(Z_pinv, Z, H)\n",
    "                A = np.concatenate([Z, H], axis=1)\n",
    "                return A, A_pinv\n",
    "\n",
    "    # ultimate fallback\n",
    "    A = np.concatenate([Z, H], axis=1)\n",
    "    return A, pinv_ridge_np(A, lam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747f3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- train & eval (GPU-aware) ---------------- #\n",
    "def train_and_eval(updater, npw, mw, seed=SEED):\n",
    "    set_seed(seed)\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    # data\n",
    "    X_train, y_train, X_test, y_test = load_mnist(flatten=True)\n",
    "    Y_train = to_onehot(y_train, 10)\n",
    "\n",
    "    if USE_GPU:\n",
    "        # ---- GPU path ----\n",
    "        Xtr_t = torch.from_numpy(X_train).to(DEVICE)\n",
    "        Xte_t = torch.from_numpy(X_test).to(DEVICE)\n",
    "\n",
    "        Ws_f_t, bs_f_t = init_feature_layer_torch(Xtr_t.shape[1], npw, NW, rng, DEVICE)\n",
    "        Z_t = forward_feature_layer_torch(Xtr_t, Ws_f_t, bs_f_t, SPIKE_STEPS)\n",
    "        Ws_h_t, bs_h_t = init_enh_layer_torch(Z_t.shape[1], MP, mw, rng, DEVICE)\n",
    "        H_t = forward_enh_layer_torch(Z_t, Ws_h_t, bs_h_t, SPIKE_STEPS)\n",
    "\n",
    "        mem_before = peak_mem_mb()\n",
    "        with Timer() as t:\n",
    "            if updater == \"direct\":\n",
    "                # full GPU solve\n",
    "                A_t = torch.cat([Z_t, H_t], dim=1)\n",
    "                A_pinv_t = pinv_ridge_torch(A_t, lam=max(RIDGE, 1e-3))\n",
    "                Ytr_t = torch.from_numpy(Y_train).to(DEVICE)\n",
    "                W_out_t = A_pinv_t @ Ytr_t\n",
    "            else:\n",
    "                # greville / updated_greville on CPU (NumPy); forward done on GPU\n",
    "                Z = Z_t.detach().cpu().numpy()\n",
    "                H = H_t.detach().cpu().numpy()\n",
    "                Z_pinv = pinv_ridge_np(Z, lam=max(RIDGE, 1e-3))\n",
    "                A, A_pinv = extend_columns_with_backend(updater, Z, Z_pinv, H, lam=max(RIDGE, 1e-3))\n",
    "                W_out = (A_pinv @ Y_train).astype(np.float32)\n",
    "        train_time = t.elapsed\n",
    "        mem_after = peak_mem_mb()\n",
    "        peak_mem = max(mem_before, mem_after)\n",
    "\n",
    "        # eval\n",
    "        Zte_t = forward_feature_layer_torch(Xte_t, Ws_f_t, bs_f_t, SPIKE_STEPS)\n",
    "        Hte_t = forward_enh_layer_torch(Zte_t, Ws_h_t, bs_h_t, SPIKE_STEPS)\n",
    "        if updater == \"direct\":\n",
    "            At_t = torch.cat([Zte_t, Hte_t], dim=1)\n",
    "            logits_t = At_t @ W_out_t\n",
    "            y_pred = logits_t.argmax(dim=1).detach().cpu().numpy()\n",
    "        else:\n",
    "            At = np.concatenate([Zte_t.detach().cpu().numpy(), Hte_t.detach().cpu().numpy()], axis=1).astype(np.float32)\n",
    "            logits = At @ W_out\n",
    "            y_pred = np.argmax(logits, axis=1)\n",
    "\n",
    "        acc = float((y_pred == y_test).mean())\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=list(range(10)))\n",
    "        fname = f\"confmat_{updater}_np{npw}_mw{mw}.png\"\n",
    "        plot_confusionmatrix(cm, f\"{updater}  np={npw}  mw={mw}\", os.path.join(SAVE_DIR, fname))\n",
    "        return acc, train_time, peak_mem\n",
    "\n",
    "     # ---- CPU path (original) ----\n",
    "    Ws_f, bs_f = init_feature_layer(in_dim=X_train.shape[1], np_per_win=npw, nw=NW, rng=rng)\n",
    "    Z = forward_feature_layer(X_train, Ws_f, bs_f, steps=SPIKE_STEPS, rng=rng)\n",
    "    Ws_h, bs_h = init_enh_layer(feat_dim=Z.shape[1], mp_per_win=MP, mw=mw, rng=rng)\n",
    "    H = forward_enh_layer(Z, Ws_h, bs_h, steps=SPIKE_STEPS, rng=rng)\n",
    "    mem_before = peak_mem_mb()\n",
    "    with Timer() as t:\n",
    "        Z_pinv = pinv_ridge_np(Z, lam=max(RIDGE, 1e-3))\n",
    "        A, A_pinv = extend_columns_with_backend(updater, Z, Z_pinv, H, lam=max(RIDGE, 1e-3))\n",
    "        W_out = (A_pinv @ to_onehot(y_train, 10)).astype(np.float32)\n",
    "    train_time = t.elapsed\n",
    "    mem_after = peak_mem_mb()\n",
    "    peak_mem = max(mem_before, mem_after)\n",
    "    Zt = forward_feature_layer(X_test, Ws_f, bs_f, steps=SPIKE_STEPS, rng=rng)\n",
    "    Ht = forward_enh_layer(Zt, Ws_h, bs_h, steps=SPIKE_STEPS, rng=rng)\n",
    "    At = np.concatenate([Zt, Ht], axis=1).astype(np.float32)\n",
    "    logits = At @ W_out\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    acc = float((y_pred == y_test).mean())\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=list(range(10)))\n",
    "    fname = f\"confmat_{updater}_np{npw}_mw{mw}.png\"\n",
    "    plot_confusionmatrix(cm, f\"{updater}  np={npw}  mw={mw}\", os.path.join(SAVE_DIR, fname))\n",
    "    return acc, train_time, peak_mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c3d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------main-------------------#\n",
    "# ===================== E1 ===================== #\n",
    "def experiment_1():\n",
    "    ensure_dirs()\n",
    "    print(\"== E1: Capacity Sweep (np in {50,100}, mw in {10,20}) ==\")\n",
    "    results = []\n",
    "    for updater in UPDATERS:\n",
    "        print(f\"\\n-- Updater: {updater} --\")\n",
    "        for npw in NPS:\n",
    "            for mw in MWS:\n",
    "                acc, tsec, pmem = train_and_eval(updater, npw, mw, seed=SEED)\n",
    "                print(f\"np={npw:3d}  mw={mw:3d}  |  acc={acc*100:5.2f}%  time={tsec:6.2f}s  peak_mem≈{pmem:7.2f} MB\")\n",
    "                results.append(dict(updater=updater, np=npw, mw=mw, acc=acc, time=tsec, peak_mem_mb=pmem))\n",
    "    with open(\"e1_minimal_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "191f6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== E2 (updated_greville only) ===================== #\n",
    "def experiment_2(\n",
    "    train_sizes=(5_000, 10_000, 30_000, 60_000),\n",
    "    seeds=tuple(range(10)),      # 10 runs (iterations)\n",
    "    npw=100, mw=20\n",
    "):\n",
    "    \"\"\"\n",
    "    E2: Influence of training set size using only updated_greville.\n",
    "    Outputs:\n",
    "      - Console table (k vs mean accuracy % over seeds)\n",
    "      - CSV: results/e2_table_accuracy_updated.csv\n",
    "      - Figure (dots+line): figures/e2_acc_vs_k_updated.png\n",
    "    \"\"\"\n",
    "    ensure_dirs()\n",
    "    print(\"\\n== E2: Training Set Size (updated_greville) ==\")\n",
    "    X_tr_full, y_tr_full, X_te, y_te = load_mnist(flatten=True)\n",
    "\n",
    "    # stratified subset for balance & reproducibility\n",
    "    def _k_indices_stratified(y: np.ndarray, k: int, seed: int) -> np.ndarray:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        per_class = {c: np.where(y == c)[0] for c in range(10)}\n",
    "        for c in per_class: rng.shuffle(per_class[c])\n",
    "        base, rem = k // 10, k % 10\n",
    "        counts = [base + (1 if i < rem else 0) for i in range(10)]\n",
    "        idx = np.concatenate([per_class[c][:counts[c]] for c in range(10)], axis=0)\n",
    "        rng.shuffle(idx)\n",
    "        return idx\n",
    "\n",
    "    rows = []\n",
    "    for k in train_sizes:\n",
    "        accs = []\n",
    "        for seed in seeds:\n",
    "            set_seed(seed)\n",
    "            rng = np.random.RandomState(seed)\n",
    "            idx = _k_indices_stratified(y_tr_full, k, seed)\n",
    "            X_tr = X_tr_full[idx]; y_tr = y_tr_full[idx]\n",
    "            Y_tr = to_onehot(y_tr, 10)\n",
    "\n",
    "            if USE_GPU:\n",
    "                Xtr_t = torch.from_numpy(X_tr).to(DEVICE)\n",
    "                Xte_t = torch.from_numpy(X_te).to(DEVICE)\n",
    "\n",
    "                Ws_f_t, bs_f_t = init_feature_layer_torch(Xtr_t.shape[1], npw, NW, rng, DEVICE)\n",
    "                Z_t = forward_feature_layer_torch(Xtr_t, Ws_f_t, bs_f_t, SPIKE_STEPS)\n",
    "                Ws_h_t, bs_h_t = init_enh_layer_torch(Z_t.shape[1], MP, mw, rng, DEVICE)\n",
    "                H_t = forward_enh_layer_torch(Z_t, Ws_h_t, bs_h_t, SPIKE_STEPS)\n",
    "\n",
    "                # updater on CPU (NumPy)\n",
    "                Z = Z_t.detach().cpu().numpy()\n",
    "                H = H_t.detach().cpu().numpy()\n",
    "                lam = max(RIDGE, 1e-3)\n",
    "                Z_pinv = pinv_ridge_np(Z, lam=lam)\n",
    "                A, A_pinv = extend_columns_with_backend(\"updated_greville\", Z, Z_pinv, H, lam=lam)\n",
    "                W_out = (A_pinv @ Y_tr).astype(np.float32)\n",
    "\n",
    "                # eval\n",
    "                Zte_t = forward_feature_layer_torch(Xte_t, Ws_f_t, bs_f_t, SPIKE_STEPS)\n",
    "                Hte_t = forward_enh_layer_torch(Zte_t, Ws_h_t, bs_h_t, SPIKE_STEPS)\n",
    "                At = np.concatenate([Zte_t.detach().cpu().numpy(), Hte_t.detach().cpu().numpy()], axis=1).astype(np.float32)\n",
    "                y_pred = np.argmax(At @ W_out, axis=1)\n",
    "\n",
    "            else:\n",
    "                # CPU path\n",
    "                Ws_f, bs_f = init_feature_layer(in_dim=X_tr.shape[1], np_per_win=npw, nw=NW, rng=rng)\n",
    "                Z = forward_feature_layer(X_tr, Ws_f, bs_f, steps=SPIKE_STEPS, rng=rng)\n",
    "                Ws_h, bs_h = init_enh_layer(feat_dim=Z.shape[1], mp_per_win=MP, mw=mw, rng=rng)\n",
    "                H = forward_enh_layer(Z, Ws_h, bs_h, steps=SPIKE_STEPS, rng=rng)\n",
    "\n",
    "                lam = max(RIDGE, 1e-3)\n",
    "                Z_pinv = pinv_ridge_np(Z, lam=lam)\n",
    "                A, A_pinv = extend_columns_with_backend(\"updated_greville\", Z, Z_pinv, H, lam=lam)\n",
    "                W_out = (A_pinv @ Y_tr).astype(np.float32)\n",
    "\n",
    "                Zt = forward_feature_layer(X_te, Ws_f, bs_f, steps=SPIKE_STEPS, rng=rng)\n",
    "                Ht = forward_enh_layer(Zt, Ws_h, bs_h, steps=SPIKE_STEPS, rng=rng)\n",
    "                At = np.concatenate([Zt, Ht], axis=1).astype(np.float32)\n",
    "                y_pred = np.argmax(At @ W_out, axis=1)\n",
    "\n",
    "            acc = float((y_pred == y_te).mean())\n",
    "            accs.append(acc)\n",
    "            rows.append(dict(k=k, seed=seed, acc=acc))\n",
    "\n",
    "        print(f\"k={k:6d} | acc={np.mean(accs)*100:5.2f}% ± {np.std(accs)*100:4.2f}\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    table = (\n",
    "        df.groupby(\"k\")[\"acc\"]\n",
    "          .mean()\n",
    "          .mul(100)\n",
    "          .round(2)\n",
    "          .reset_index(name=\"accuracy_pct\")\n",
    "          .sort_values(\"k\")\n",
    "    )\n",
    "    table.to_csv(\"results/e2_table_accuracy_updated.csv\", index=False)\n",
    "    print(\"Saved: results/e2_table_accuracy_updated.csv\")\n",
    "\n",
    "    x = table[\"k\"].values\n",
    "    y = table[\"accuracy_pct\"].values\n",
    "    plt.figure(figsize=(7,4), dpi=140)\n",
    "    plt.plot(x, y, marker=\"o\", linestyle=\"-\")   # dots + line\n",
    "    plt.xlabel(\"Training set size (k samples)\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"E2 (updated_greville): Accuracy vs Training Size\")\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/e2_acc_vs_k_updated.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved: figures/e2_acc_vs_k_updated.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9340106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== E3 ===================== #\n",
    "def experiment_3(\n",
    "    seeds=tuple(range(10)),    # 10 runs\n",
    "    npw=100,                   # fixed feature size\n",
    "    mp=MP,                     # 100 per window\n",
    "    base_mw=20,\n",
    "    target_mw=30,\n",
    "    schedules=dict(\n",
    "        V1=[2,2,2,2,2],       # 20→22→24→26→28→30\n",
    "        V2=[5,5],             # 20→25→30\n",
    "        V3=[10],              # 20→30\n",
    "    ),\n",
    "    ridge=None,                # if None -> use max(RIDGE, 1e-3)\n",
    "):\n",
    "    \"\"\"\n",
    "    E3: Incremental column updates by adding enhancement windows.\n",
    "    Compares 'direct', 'greville', 'updated_greville' across schedules.\n",
    "    Outputs: rows CSV + per-schedule accuracy curves.\n",
    "    \"\"\"\n",
    "    ensure_dirs()\n",
    "    lam = max(RIDGE, 1e-3) if ridge is None else ridge\n",
    "\n",
    "    X_tr_full, y_tr_full, X_te, y_te = load_mnist(flatten=True)\n",
    "    Y_tr_full = to_onehot(y_tr_full, 10)\n",
    "\n",
    "    def make_blocks(feat_dim, inc_list, rng):\n",
    "        blocks = []\n",
    "        for dmw in inc_list:\n",
    "            if USE_GPU:\n",
    "                # we only need weights (they'll be used for torch or numpy forward)\n",
    "                Ws_blk, bs_blk = init_enh_layer(feat_dim=feat_dim, mp_per_win=mp, mw=dmw, rng=rng)\n",
    "            else:\n",
    "                Ws_blk, bs_blk = init_enh_layer(feat_dim=feat_dim, mp_per_win=mp, mw=dmw, rng=rng)\n",
    "            blocks.append((Ws_blk, bs_blk))\n",
    "        return blocks\n",
    "\n",
    "    rows = []  # dict(method, schedule, total_mw, seed, acc, step_time)\n",
    "\n",
    "    print(\"\\n== E3: Incremental Enhancement Nodes (Column Updates) ==\")\n",
    "    for schedule_name, inc_list in schedules.items():\n",
    "        print(f\"\\n-- Schedule {schedule_name}: base={base_mw}, increments={inc_list}, target={target_mw} --\")\n",
    "\n",
    "        for seed in seeds:\n",
    "            set_seed(seed)\n",
    "            rng = np.random.RandomState(seed)\n",
    "\n",
    "            # Features (Z) once per seed\n",
    "            if USE_GPU:\n",
    "                Xtr_t = torch.from_numpy(X_tr_full).to(DEVICE)\n",
    "                Xte_t = torch.from_numpy(X_te).to(DEVICE)\n",
    "                Ws_f_t, bs_f_t = init_feature_layer_torch(Xtr_t.shape[1], npw, NW, rng, DEVICE)\n",
    "                Z_t = forward_feature_layer_torch(Xtr_t, Ws_f_t, bs_f_t, SPIKE_STEPS)\n",
    "                Z = Z_t.detach().cpu().numpy()\n",
    "            else:\n",
    "                Ws_f, bs_f = init_feature_layer(in_dim=X_tr_full.shape[1], np_per_win=npw, nw=NW, rng=rng)\n",
    "                Z = forward_feature_layer(X_tr_full, Ws_f, bs_f, steps=SPIKE_STEPS, rng=rng)\n",
    "\n",
    "            # Base enhancement windows\n",
    "            Ws_base, bs_base = init_enh_layer(feat_dim=Z.shape[1], mp_per_win=mp, mw=base_mw, rng=rng)\n",
    "\n",
    "            # Pre-generate increment blocks\n",
    "            blocks = make_blocks(feat_dim=Z.shape[1], inc_list=inc_list, rng=rng)\n",
    "\n",
    "            for method in (\"direct\", \"greville\", \"updated_greville\"):\n",
    "                Ws_cum = list(Ws_base)\n",
    "                bs_cum = list(bs_base)\n",
    "                Z_pinv = pinv_ridge_np(Z, lam=lam)  # constant per seed/method\n",
    "\n",
    "                total_mw_running = base_mw\n",
    "                for (Ws_blk, bs_blk) in blocks:\n",
    "                    Ws_cum += Ws_blk\n",
    "                    bs_cum += bs_blk\n",
    "                    total_mw_running += len(Ws_blk)\n",
    "\n",
    "                    with Timer() as t_step:\n",
    "                        # Build cumulative H\n",
    "                        if USE_GPU:\n",
    "                            # forward H on GPU for speed, then back to numpy if needed\n",
    "                            Z_t = torch.from_numpy(Z).to(DEVICE)\n",
    "                            # re-create torch weights for H_cum from numpy weights:\n",
    "                            # (fast path: compute H via numpy to keep it simple & consistent)\n",
    "                            H_cum = forward_enh_layer(Z, Ws_cum, bs_cum, steps=SPIKE_STEPS, rng=rng)\n",
    "                        else:\n",
    "                            H_cum = forward_enh_layer(Z, Ws_cum, bs_cum, steps=SPIKE_STEPS, rng=rng)\n",
    "\n",
    "                        # Update A^+ and solve\n",
    "                        A, A_pinv = extend_columns_with_backend(method, Z, Z_pinv, H_cum, lam=lam)\n",
    "                        W_out = (A_pinv @ Y_tr_full).astype(np.float32)\n",
    "                    step_time = t_step.elapsed\n",
    "\n",
    "                    # Evaluate on test\n",
    "                    if USE_GPU:\n",
    "                        # compute Zt/Ht with torch if we had torch feature weights\n",
    "                        Zt_np = None\n",
    "                        if 'Ws_f_t' in locals():\n",
    "                            Zt_t = forward_feature_layer_torch(Xte_t, Ws_f_t, bs_f_t, SPIKE_STEPS)\n",
    "                            # build torch enh weights from numpy Ws_cum/bs_cum? simplest: reuse numpy path for eval too\n",
    "                            Zt_np = Zt_t.detach().cpu().numpy()\n",
    "                            Ht = forward_enh_layer(Zt_np, Ws_cum, bs_cum, steps=SPIKE_STEPS, rng=rng)\n",
    "                            At = np.concatenate([Zt_np, Ht], axis=1).astype(np.float32)\n",
    "                        else:\n",
    "                            # fallback to numpy path\n",
    "                            if 'Ws_f' in locals():\n",
    "                                Zt = forward_feature_layer(X_te, Ws_f, bs_f, steps=SPIKE_STEPS, rng=rng)\n",
    "                                Ht = forward_enh_layer(Zt, Ws_cum, bs_cum, steps=SPIKE_STEPS, rng=rng)\n",
    "                                At = np.concatenate([Zt, Ht], axis=1).astype(np.float32)\n",
    "                            else:\n",
    "                                raise RuntimeError(\"Missing feature weights for eval.\")\n",
    "                    else:\n",
    "                        Zt = forward_feature_layer(X_te, Ws_f, bs_f, steps=SPIKE_STEPS, rng=rng)\n",
    "                        Ht = forward_enh_layer(Zt, Ws_cum, bs_cum, steps=SPIKE_STEPS, rng=rng)\n",
    "                        At = np.concatenate([Zt, Ht], axis=1).astype(np.float32)\n",
    "\n",
    "                    y_pred = np.argmax(At @ W_out, axis=1)\n",
    "                    acc = float((y_pred == y_te).mean())\n",
    "\n",
    "                    rows.append(dict(\n",
    "                        schedule=schedule_name,\n",
    "                        method=method,\n",
    "                        seed=seed,\n",
    "                        total_mw=total_mw_running,\n",
    "                        acc=acc,\n",
    "                        step_time=step_time\n",
    "                    ))\n",
    "\n",
    "        # summary at final capacity\n",
    "        df_sched = pd.DataFrame([r for r in rows if r[\"schedule\"] == schedule_name])\n",
    "        df_final = df_sched[df_sched[\"total_mw\"] == target_mw]\n",
    "        print(\"  Final (mw=30) accuracy mean ± std:\")\n",
    "        for m in (\"direct\", \"greville\", \"updated_greville\"):\n",
    "            sub = df_final[df_final[\"method\"] == m][\"acc\"]\n",
    "            if not sub.empty:\n",
    "                print(f\"    {m:16s} {sub.mean()*100:5.2f}% ± {sub.std()*100:4.2f}%\")\n",
    "\n",
    "    # Aggregate & Plot\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"acc_pct\"] = df[\"acc\"] * 100\n",
    "    df.sort_values([\"schedule\",\"method\",\"total_mw\",\"seed\"]).to_csv(\"results/e3_rows.csv\", index=False)\n",
    "    print(\"Saved: results/e3_rows.csv\")\n",
    "\n",
    "    for schedule_name in schedules.keys():\n",
    "        d = df[df[\"schedule\"] == schedule_name]\n",
    "        if d.empty: \n",
    "            continue\n",
    "        plt.figure(figsize=(7,4), dpi=140)\n",
    "        for m in (\"direct\",\"greville\",\"updated_greville\"):\n",
    "            sub = (d[d[\"method\"] == m]\n",
    "                   .groupby(\"total_mw\")[\"acc_pct\"].mean()\n",
    "                   .reset_index()\n",
    "                   .sort_values(\"total_mw\"))\n",
    "            if sub.empty: \n",
    "                continue\n",
    "            plt.plot(sub[\"total_mw\"], sub[\"acc_pct\"], marker=\"o\", label=m)\n",
    "        plt.xlabel(\"Total enhancement windows (mw)\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.title(f\"E3: Accuracy vs mw — {schedule_name}\")\n",
    "        plt.grid(alpha=0.2); plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(f\"figures/e3_acc_vs_mw_{schedule_name}.png\"); plt.close()\n",
    "        print(f\"Saved: figures/e3_acc_vs_mw_{schedule_name}.png\")\n",
    "\n",
    "    df_final = df[df[\"total_mw\"] == target_mw]\n",
    "    if not df_final.empty:\n",
    "        print(\"\\nE3 — Final step time (mean ± std) [per-step at final increment]\")\n",
    "        for schedule_name in schedules.keys():\n",
    "            for m in (\"direct\",\"greville\",\"updated_greville\"):\n",
    "                sub = df_final[(df_final[\"schedule\"]==schedule_name) & (df_final[\"method\"]==m)][\"step_time\"]\n",
    "                if not sub.empty:\n",
    "                    print(f\"{schedule_name:>3s} | {m:16s} time={sub.mean():.2f}s ± {sub.std():.2f}s\")\n",
    "\n",
    "    print(\"\\n[E3] Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18e2bf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== E2: Training Set Size (updated_greville) ==\n",
      "k=  5000 | acc=70.69% ± 1.13\n",
      "k= 10000 | acc=79.08% ± 0.83\n",
      "k= 30000 | acc=83.15% ± 0.59\n",
      "k= 60000 | acc=83.83% ± 0.70\n",
      "Saved: results/e2_table_accuracy_updated.csv\n",
      "Saved: figures/e2_acc_vs_k_updated.png\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Runner ---------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    # experiment_1()\n",
    "    experiment_2()\n",
    "    # experiment_3()\n",
    "    # print(\"Ready. Uncomment an experiment() at the bottom to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
